# Housing Prices Competition for Kaggle Learn Users

This repository contains a complete data science and machine learning solution for the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/competitions/home-data-for-ml-course/overview) competition on Kaggle. The goal is to predict the final sales price of each home in Ames, Iowa.

This solution leverages **KNN Imputation** for smart missing value filling, **robust statistical preprocessing**, **skewness correction** and an **Optimized ElasticNet Pipeline** to minimize prediction error.

---

## Project Overview

* **Competition:** Housing Prices Competition for Kaggle Learn Users
* **Model Used:** ElasticNetCV (Lasso + Ridge Regularization)
* **Key Techniques:** KNN Imputation, Log-Transformation, Outlier Capping, Robust Scaling.
* **Evaluation Metric:** Root Mean Squared Error (RMSE) on Log-Transformed Prices.

---

## Key Methodologies

### 1. Data Cleaning & Advanced Imputation
Unlike basic approaches that strictly use means/medians, this notebook employs advanced imputation strategies:
* **KNN Imputation:** Utilized `KNNImputer` (K-Nearest Neighbors) to intelligently fill missing values in critical features (like `LotFrontage`). Instead of a global average, the model looks at the "5 nearest neighbors" (similar houses) to estimate the missing value accurately.
* **Categorical Handling:** Irrelevant columns were removed by looking at their scatterplot vs `SalePrice`. The columns with few missing values were filled with clever manipulation and by looking at possible relationship with the other fatures
* **Target Normalization:** The target `SalePrice` is log-transformed (`np.log1p`) to correct skewness, satisfying the normality assumption required by linear models.

### 2. Feature Engineering & Preprocessing
Raw data was transformed into high-signal features:
* **Multicollinearity Prevention:** Features were checked for correlation between each other and the features strongly correlated with each other were removed. 
* **Outlier Management:** Extreme outliers were removed from the training set, and extreme values in the test set were capped to prevent model instability.
* **Skewness Correction:** Numerical features with high skewness (> 0.75) were log-transformed to improve model performance.
* **Robust Scaling:** A `RobustScaler` was applied within the pipeline to scale features using statistics that are robust to outliers (Median and IQR), ensuring extreme values don't distort the model.

### 3. Modeling Pipeline
The final model utilizes a fully automated `scikit-learn` Pipeline:
* **Algorithm:** **ElasticNetCV**
* **Why ElasticNet?** It combines the feature selection capabilities of Lasso (L1) with the coefficient stability of Ridge (L2). This is ideal for housing data where many features are correlated (multicollinearity).
* **Cross-Validation:** The model automatically performs 5-fold cross-validation to tune the `alpha` (penalty strength) and `l1_ratio` parameters.
* **Inverse Transformation:** Final predictions are automatically converted from log-scale back to real dollars using `np.expm1`.

---

## File Structure

```text
├── House-Pricing.ipynb       # Main Jupyter Notebook containing Data Cleaning, KNN, and Modeling
├── submission.csv            # Final predictions generated by the model
├── data_description.txt      # Description of the Dataset i.e. column names and variable names
├── test.csv                  # Test dataset
├── train.csv                 # Train dataset
└── README.md                 # Project documentation
```

##  Accuracy Score

This notebook received an accuracy score(RMSE) of **15138.74030** and a rank of **400** out of **4368** in a rolling leaderboard.

## Dependencies

To run this code locally, you need the following Python libraries:

```python
pip install numpy pandas matplotlib seaborn scipy scikit-learn
```

Also you need to upload `test.csv` and `train.csv` in the same folder where you run the `House-Pricing.ipynb` notebook.
